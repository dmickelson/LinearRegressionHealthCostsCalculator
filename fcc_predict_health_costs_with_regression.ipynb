{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rRo8oNqZ-Rj"
      },
      "outputs": [],
      "source": [
        "# Import libraries. You may or may not use all of these.\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiX2FI4gZtTt"
      },
      "outputs": [],
      "source": [
        "# Import data\n",
        "!wget https://cdn.freecodecamp.org/project-data/health-costs/insurance.csv\n",
        "dataset = pd.read_csv('insurance.csv')\n",
        "dataset.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LcopvQh3X-kX"
      },
      "outputs": [],
      "source": [
        "# prompt: Make sure to convert categorical data to numbers within the dataset\n",
        "\n",
        "dataset['sex'] = pd.factorize(dataset['sex'])[0]\n",
        "dataset['smoker'] = pd.factorize(dataset['smoker'])[0]\n",
        "dataset['region'] = pd.factorize(dataset['region'])[0]\n",
        "dataset.tail()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Use 80% of the data as the train_dataset and 20% of the data as the test_dataset.\n",
        "\n",
        "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
        "test_dataset = dataset.drop(train_dataset.index)\n"
      ],
      "metadata": {
        "id": "bWtu7_DGQg-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspect the data\n",
        "Review the joint distribution of a few pairs of columns from the training set."
      ],
      "metadata": {
        "id": "pVnZlPG2w_bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.pairplot(train_dataset[['expenses', 'age', 'bmi']], diag_kind='kde')"
      ],
      "metadata": {
        "id": "k5LPHuRrwjH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also check the overall statistics. Note how each feature covers a very different range:"
      ],
      "metadata": {
        "id": "slbmajrvxKS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.describe().transpose()"
      ],
      "metadata": {
        "id": "5vy6PYHxxGSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split features from labels\n",
        "Separate the target value—the \"label\"—from the features. This label is the value that you will train the model to predict."
      ],
      "metadata": {
        "id": "uj2kp9gXxbp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_features = train_dataset.copy()\n",
        "test_features = test_dataset.copy()\n",
        "\n",
        "train_labels = train_features.pop('expenses')\n",
        "test_labels = test_features.pop('expenses')"
      ],
      "metadata": {
        "id": "cvBmfSd2yFsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "In the table of statistics it's easy to see how different the ranges of each feature are:"
      ],
      "metadata": {
        "id": "zfG0TbVsx1Cc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.describe().transpose()[['mean', 'std']]"
      ],
      "metadata": {
        "id": "MKTju1IKx8HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good practice to normalize features that use different scales and ranges.\n",
        "\n",
        "One reason this is important is because the features are multiplied by the model weights. So, the scale of the outputs and the scale of the gradients are affected by the scale of the inputs.\n",
        "\n",
        "Although a model might converge without feature normalization, normalization makes training much more stable."
      ],
      "metadata": {
        "id": "IF--ogus8DVb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Normalization layer\n",
        "The tf.keras.layers.Normalization is a clean and simple way to add feature normalization into your model.\n",
        "\n",
        "The first step is to create the layer:"
      ],
      "metadata": {
        "id": "WkvhNkrp8IYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = tf.keras.layers.Normalization(axis=-1)"
      ],
      "metadata": {
        "id": "YhlcliON8MP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, fit the state of the preprocessing layer to the data by calling `Normalization.adapt`:"
      ],
      "metadata": {
        "id": "TmITxmjO8QOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer.adapt(np.array(train_features))"
      ],
      "metadata": {
        "id": "p36enffJ8UTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the mean and variance, and store them in the layer:"
      ],
      "metadata": {
        "id": "6d8WUkMZ8ngx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(normalizer.mean.numpy())"
      ],
      "metadata": {
        "id": "a3TxA30W8oXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When the layer is called, it returns the input data, with each feature independently normalized:"
      ],
      "metadata": {
        "id": "4Jy5rffW8t81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first = np.array(train_features[:1])\n",
        "\n",
        "with np.printoptions(precision=2, suppress=True):\n",
        "  print('First example:', first)\n",
        "  print()\n",
        "  print('Normalized:', normalizer(first).numpy())"
      ],
      "metadata": {
        "id": "9ONXiq8m8xJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear regression"
      ],
      "metadata": {
        "id": "8ne2Si-cBH2j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building a deep neural network model, start with linear regression using one and several variables."
      ],
      "metadata": {
        "id": "w4E_z4NzIyqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = {}"
      ],
      "metadata": {
        "id": "bnX3vCnaGpGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with one variable\n",
        "Begin with a single-variable linear regression to predict 'expenses' from 'bmi'.\n",
        "\n",
        "Training a model with `tf.keras` typically starts by defining the model architecture. Use a `tf.keras.Sequential` model, which represents a sequence of steps.\n",
        "\n",
        "There are two steps in your single-variable linear regression model:\n",
        "- Normalize the 'bmi' input features using the tf.keras.layers.Normalization preprocessing layer.\n",
        "- Apply a linear transformation (\n",
        ") to produce 1 output using a linear layer (tf.keras.layers.Dense).\n",
        "\n",
        "The number of inputs can either be set by the input_shape argument, or automatically when the model is run for the first time.\n",
        "\n",
        "First, create a NumPy array made of the `bmi` features. Then, instantiate the `tf.keras.layers.Normalization` and fit its state to the `bmi` data:"
      ],
      "metadata": {
        "id": "WMZ2MCk9I5BK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "bmi = np.array(train_features['bmi'])\n",
        "\n",
        "bmi_normalizer = layers.Normalization(axis=None)\n",
        "bmi_normalizer.adapt(bmi)"
      ],
      "metadata": {
        "id": "PffOfiPwJgyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the Keras Sequential model:"
      ],
      "metadata": {
        "id": "uLAIOdvaKKJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmi_model = tf.keras.Sequential([\n",
        "    Input(shape=(1,)),\n",
        "    bmi_normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])\n",
        "\n",
        "bmi_model.summary()"
      ],
      "metadata": {
        "id": "iBVClgI4KKzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model will predict 'expenses' from 'bmi'.\n",
        "\n",
        "Run the untrained model on the first 10 'bmi' values. The output won't be good, but notice that it has the expected shape of (10, 1):"
      ],
      "metadata": {
        "id": "yEzV8uBLKZxT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmi_model.predict(bmi[:10])"
      ],
      "metadata": {
        "id": "XyZlb0YuKk1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is built, configure the training procedure using the Keras `Model.compile` method. The most important arguments to compile are the `loss` and the `optimizer`, since these define what will be optimized (`mean_absolute_error`) and how (using the `tf.keras.optimizers.Adam`)."
      ],
      "metadata": {
        "id": "1uHDcZiGMN26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bmi_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "QVzBDzguMdzN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Keras `Model.fit` to execute the training for 100 epochs:"
      ],
      "metadata": {
        "id": "5UMEyfFJMjEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = bmi_model.fit(\n",
        "    train_features['bmi'],\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # Suppress logging.\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "id": "v9nGgLE0MoWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the model's training progress using the stats stored in the history object:"
      ],
      "metadata": {
        "id": "u-PQc7PcMvIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "metadata": {
        "id": "LmRTfAFLMwBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (Mean Absolute Error)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "VtvXvYOUM3mV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "FUFQoP_oM5k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results['bmi_model'] = bmi_model.evaluate(\n",
        "    test_features['bmi'],\n",
        "    test_labels, verbose=0)"
      ],
      "metadata": {
        "id": "72UrjwEENHua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since this is a single variable regression, it's easy to view the model's predictions as a function of the input:"
      ],
      "metadata": {
        "id": "hILk2VJxNR-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = bmi_model.predict(x)"
      ],
      "metadata": {
        "id": "aJWtFn14NUkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_bmi(x, y):\n",
        "  plt.scatter(train_features['bmi'], train_labels, label='Data')\n",
        "  plt.plot(x, y, color='k', label='Predictions')\n",
        "  plt.xlabel('BMI')\n",
        "  plt.ylabel('Expense')\n",
        "  plt.legend()\n"
      ],
      "metadata": {
        "id": "jgLUAQEsNi5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_bmi(x, y)"
      ],
      "metadata": {
        "id": "5YkZTKdgNp2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear regression with multiple inputs"
      ],
      "metadata": {
        "id": "7LFzyYOQBfca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ou can use an almost identical setup to make predictions based on multiple inputs. This model still does the same _y=mx+b_ calculation except that _m_ is a matrix and _x_ is a vector.\n",
        "\n",
        "Create a two-step Keras Sequential model again with the first layer being `normalizer (tf.keras.layers.Normalization(axis=-1))` you defined earlier and adapted to the whole dataset:"
      ],
      "metadata": {
        "id": "AJcT0s-fBljL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = tf.keras.Sequential([\n",
        "    normalizer,\n",
        "    layers.Dense(units=1)\n",
        "])"
      ],
      "metadata": {
        "id": "Ssuduc3fCJqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call Model.predict on a batch of inputs, it produces units=1 outputs for each example:"
      ],
      "metadata": {
        "id": "AClCXDrDCNuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.predict(train_features[:10])"
      ],
      "metadata": {
        "id": "i02DEdo5CQdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you call the model, its weight matrices will be built—check that the kernel weights (the\n",
        " in\n",
        ") have a shape of (9, 1):"
      ],
      "metadata": {
        "id": "wxCfhERCCXy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.layers[1].kernel"
      ],
      "metadata": {
        "id": "T8_-bWnHCilW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configure the model with Keras `Model.compile` and train with `Model.fit` for 100 epochs:"
      ],
      "metadata": {
        "id": "YpaxTHdZCn5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.1),\n",
        "    loss='mean_absolute_error')"
      ],
      "metadata": {
        "id": "b5njE7k6CuoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = linear_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    epochs=100,\n",
        "    # Suppress logging.\n",
        "    verbose=0,\n",
        "    # Calculate validation results on 20% of the training data.\n",
        "    validation_split = 0.2)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "T9GdFK_zCxZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using all the inputs in this regression model achieves a much lower training and validation error than the horsepower_model, which had one input:"
      ],
      "metadata": {
        "id": "f3sLViARC3UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hist = pd.DataFrame(history.history)\n",
        "hist['epoch'] = history.epoch\n",
        "hist.tail()"
      ],
      "metadata": {
        "id": "nWZR-T15Dlq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss(history):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss Over Epochs')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (Mean Absolute Error)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vdAdrOKKEAHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "a_BLfkmlC6rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect the results on the test set for later:"
      ],
      "metadata": {
        "id": "KzaYH5-QC-vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results['linear_model'] = linear_model.evaluate(\n",
        "    test_features, test_labels, verbose=0)"
      ],
      "metadata": {
        "id": "ZyNHSGS5C_98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression with a deep neural network (DNN)\n",
        "In the previous section, you implemented two linear models for single and multiple inputs.\n",
        "\n",
        "Here, you will implement single-input and multiple-input DNN models.\n",
        "\n",
        "The code is basically the same except the model is expanded to include some \"hidden\" non-linear layers. The name \"hidden\" here just means not directly connected to the inputs or outputs.\n",
        "\n",
        "These models will contain a few more layers than the linear model:\n",
        "- The normalization layer, as before (with `bmi_normalizer` for a single-input model and `normalizer` for a multiple-input model).\n",
        "- Two hidden, non-linear, Dense layers with the ReLU (`relu`) activation function nonlinearity.\n",
        "- A linear 1Dense1 single-output layer.\n",
        "\n",
        "Both models will use the same training procedure, so the `ompile` method is included in the `build_and_compile_model` function below."
      ],
      "metadata": {
        "id": "Zm_D_w5eOsen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_and_compile_model(norm):\n",
        "  model = keras.Sequential([\n",
        "      norm,\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "      layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss='mean_absolute_error',\n",
        "                optimizer=tf.keras.optimizers.Adam(0.001)\n",
        "                # metrics=['mae', 'mse']\n",
        "                )\n",
        "  return model"
      ],
      "metadata": {
        "id": "HEUclt98PIqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression using a DNN and a single input\n",
        "Create a DNN model with only 'BMI' as input and `bmi_normalizer` (defined earlier) as the normalization layer:"
      ],
      "metadata": {
        "id": "At7jZ7DhPQ40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_bmi_model = build_and_compile_model(bmi_normalizer)"
      ],
      "metadata": {
        "id": "Rnu1l8TdPfQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has quite a few more trainable parameters than the linear models:"
      ],
      "metadata": {
        "id": "9zyYmP4ePmtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_bmi_model.summary()"
      ],
      "metadata": {
        "id": "DeLIJpYpPox_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with Keras `Model.fit`:"
      ],
      "metadata": {
        "id": "sNqGl-2hQFSc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_bmi_model.fit(\n",
        "    train_features['bmi'],\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "metadata": {
        "id": "ePKRPRhGQEKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model does slightly better than the linear single-input bmi_model:"
      ],
      "metadata": {
        "id": "IW5-UNg7QVHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "p37fPx7-QW1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you plot the predictions as a function of 'BMI', you should notice how this model takes advantage of the nonlinearity provided by the hidden layers:"
      ],
      "metadata": {
        "id": "EXLSdP8RQgoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.linspace(0.0, 250, 251)\n",
        "y = dnn_bmi_model.predict(x)"
      ],
      "metadata": {
        "id": "2Y9QnYSVQkE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_bmi(x, y)"
      ],
      "metadata": {
        "id": "JBlcDgeuQqtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect the results on the test set for later:"
      ],
      "metadata": {
        "id": "LU_R1zWsQ3QI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results['dnn_bmi_model'] = dnn_bmi_model.evaluate(\n",
        "    test_features['bmi'], test_labels,\n",
        "    verbose=0)"
      ],
      "metadata": {
        "id": "Buwg61aIQ63b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression using a DNN and multiple inputs\n",
        "Repeat the previous process using all the inputs. The model's performance slightly improves on the validation dataset."
      ],
      "metadata": {
        "id": "5VCZkRSaRGkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dnn_model = build_and_compile_model(normalizer)\n",
        "dnn_model.summary()"
      ],
      "metadata": {
        "id": "bnkvECOBRKZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = dnn_model.fit(\n",
        "    train_features,\n",
        "    train_labels,\n",
        "    validation_split=0.2,\n",
        "    verbose=0, epochs=100)"
      ],
      "metadata": {
        "id": "DnFjBwjNRO-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss(history)"
      ],
      "metadata": {
        "id": "XQkd76AnRVYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collect the results on the test set:"
      ],
      "metadata": {
        "id": "ybatbqyPRZ66"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_results['dnn_model'] = dnn_model.evaluate(test_features, test_labels, verbose=0)"
      ],
      "metadata": {
        "id": "LjDt1rNnRaol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance\n",
        "Since all models have been trained, you can review their test set performance:"
      ],
      "metadata": {
        "id": "MV1ASqv2G9a2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(test_results, index=['Mean absolute error [expense]']).T"
      ],
      "metadata": {
        "id": "ICWM5lvfHCZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These results match the validation error observed during training."
      ],
      "metadata": {
        "id": "6UM1OvMOHrjT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions\n",
        "You can now make predictions with the dnn_model on the test set using Keras Model.predict and review the loss:"
      ],
      "metadata": {
        "id": "rPc_beDMHxHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = dnn_model.predict(test_features).flatten()\n",
        "# test_predictions = linear_model.predict(test_features).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True Values [expense]')\n",
        "plt.ylabel('Predictions [expense]')\n",
        "lims = [0, 10000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims, lims)"
      ],
      "metadata": {
        "id": "TyAAm6bLH1aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, check the error distribution:"
      ],
      "metadata": {
        "id": "pRtSMhBeIbLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "error = test_predictions - test_labels\n",
        "plt.hist(error, bins=25)\n",
        "plt.xlabel('Prediction Error [expense]')\n",
        "_ = plt.ylabel('Count')"
      ],
      "metadata": {
        "id": "fU0jCcFzIdBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temp reset for the last cell\n",
        "model = dnn_model\n",
        "test_dataset = test_features"
      ],
      "metadata": {
        "id": "Q3f0gV9tTNeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe7RXH3N3CWU"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL TO TEST YOUR MODEL. DO NOT MODIFY CONTENTS.\n",
        "# Test model by checking how well the model generalizes using the test set.\n",
        "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=2)\n",
        "\n",
        "print(\"Testing set Mean Abs Error: {:5.2f} expenses\".format(mae))\n",
        "\n",
        "if mae < 3500:\n",
        "  print(\"You passed the challenge. Great job!\")\n",
        "else:\n",
        "  print(\"The Mean Abs Error must be less than 3500. Keep trying.\")\n",
        "\n",
        "# Plot predictions.\n",
        "test_predictions = model.predict(test_dataset).flatten()\n",
        "\n",
        "a = plt.axes(aspect='equal')\n",
        "plt.scatter(test_labels, test_predictions)\n",
        "plt.xlabel('True values (expenses)')\n",
        "plt.ylabel('Predictions (expenses)')\n",
        "lims = [0, 50000]\n",
        "plt.xlim(lims)\n",
        "plt.ylim(lims)\n",
        "_ = plt.plot(lims,lims)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}